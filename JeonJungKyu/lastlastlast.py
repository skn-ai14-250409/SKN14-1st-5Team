import streamlit as st
import mysql.connector
from dotenv import load_dotenv
import os
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from gtts import gTTS
from io import BytesIO
from datetime import datetime
import time

# â”€â”€ 1) .env ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPEN_AI_API")

client = OpenAI(api_key=OPENAI_API_KEY)  # â­ ì‹ ë²„ì „ ìŠ¤íƒ€ì¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±

db_config = {
    'host': os.getenv('DB_HOST'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'database': os.getenv('DB_NAME')
}

if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
    raise ValueError("NAVER_CLIENT_ID and NAVER_CLIENT_SECRET ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

# â”€â”€ 2) DB ì—°ê²° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_db_connection():
    try:
        conn = mysql.connector.connect(**db_config)
        return conn
    except mysql.connector.Error as err:
        st.error(f"DB ì—°ê²° ì‹¤íŒ¨: {err}")
        return None

# â”€â”€ 3) NaverNews í´ë˜ìŠ¤ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class NaverNews:
    def __init__(self, title, link, originallink, description, pubDate):
        self.title = title
        self.link = link
        self.originallink = originallink
        self.description = description
        self.pubDate = pubDate

# â”€â”€ 4) ë‰´ìŠ¤ ì €ì¥ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_news_to_db(news_list: list, table_name: str):
    try:
        conn = get_db_connection()
        if conn:
            with conn.cursor() as cursor:
                cursor.execute(f'SELECT COUNT(*) FROM {table_name}')
                row_count = cursor.fetchone()[0]

                # ì˜¤ë˜ëœ ë‰´ìŠ¤ ì‚­ì œ
                if row_count > 3:
                    to_delete = row_count - 3
                    cursor.execute(f'SELECT id FROM {table_name} ORDER BY pub_date ASC LIMIT %s', (to_delete,))
                    old_ids = cursor.fetchall()

                    for (news_id,) in old_ids:
                        cursor.execute(f'DELETE FROM {table_name} WHERE id = %s', (news_id,))
                    conn.commit()
                    st.info(f"âœ… ë‰´ìŠ¤ê°€ 3ê°œë¥¼ ë„˜ì–´ {to_delete}ê°œì˜ ì˜¤ë˜ëœ ë‰´ìŠ¤ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤!")

                # ì¤‘ë³µëœ ë‰´ìŠ¤ URL í™•ì¸í•˜ê³  ì €ì¥
                for news in news_list:
                    # ì¤‘ë³µëœ originallinkê°€ ìˆëŠ”ì§€ í™•ì¸
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name} WHERE originallink = %s", (news.originallink,))
                    existing_news_count = cursor.fetchone()[0]

                    if existing_news_count == 0:  # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ìƒˆë¡œìš´ ë‰´ìŠ¤ ì¶”ê°€
                        cursor.execute(f'''
                            INSERT INTO {table_name} (title, originallink, link, description, pub_date)
                            VALUES (%s, %s, %s, %s, %s)
                        ''', (news.title, news.originallink, news.link, news.description, news.pubDate))
                conn.commit()
    except mysql.connector.Error as e:
        st.error(f"DB ì˜¤ë¥˜: {e}")


# â”€â”€ 5) ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_news(keyword: str) -> list:
    url = 'https://openapi.naver.com/v1/search/news.json'
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {
        'query': f'{keyword} êµí†µì‚¬ê³ ',
        'display': 3,
        'start': 1,
        'sort': 'sim',
    }

    try:
        response = requests.get(url, headers=headers, params=params)
        naver_news_list = []

        if response.status_code == 200:
            data = response.json()
            items = data.get('items', [])
            for item in items:
                naver_news_list.append(NaverNews(**item))
        else:
            st.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")

        if not naver_news_list:
            st.warning("í•´ë‹¹ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        return naver_news_list
    except Exception as e:
        st.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

# â”€â”€ 6) HTML ì „ì²˜ë¦¬ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_html(soup: BeautifulSoup):
    for tag in soup(['script', 'style', 'iframe', 'footer', 'nav', 'header', '.ad', '.related-article']):
        tag.decompose()

    for tag in soup():
        del tag['style']
        del tag['class']

# â”€â”€ 7) ë‰´ìŠ¤ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_html_text(url: str) -> str:
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        clean_html(soup)
        return soup.get_text(separator="\n")
    except Exception as e:
        st.error(f"ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        return ""

# â”€â”€ 8) ë‰´ìŠ¤ ìš”ì•½ ê¸°ëŠ¥ (ì‹ ë²„ì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_text(text: str) -> str:
    try:
        lines = text.splitlines()

        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if len(line) > 30:
                cleaned_lines.append(line)

        cleaned_text = "\n".join(cleaned_lines)

        max_chars = 8000
        if len(cleaned_text) > max_chars:
            cleaned_text = cleaned_text[:max_chars]

        prompt = f"""
        ë‹¤ìŒ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ì¤‘ìš” ê¸°ì‚¬ ë‚´ìš©ë§Œ 3ì¤„ë¡œ ìš”ì•½í•´ì¤˜.
        ë©”ë‰´, ê´‘ê³ , ëŒ“ê¸€, ì €ì‘ê¶Œ ë¬¸êµ¬ëŠ” ë¬´ì‹œí•´ì¤˜.

        {cleaned_text}
        """

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
        )

        summary = response.choices[0].message.content.strip()
        return summary

    except Exception as e:
        print(f"ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

# â”€â”€ 9) Streamlit ì•± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ“° ì§€ì—­ë³„ êµí†µì‚¬ê³  ë‰´ìŠ¤ ìš”ì•½ & TTS")

# ë“œë¡­ë‹¤ìš´
departure = st.selectbox("ì¶œë°œì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”", "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ"])
transit = st.selectbox("ê²½ìœ ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”", "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ"])
destination = st.selectbox("ëª©ì ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”", "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ"])

# ë“œë¡­ë‹¤ìš´ ì„ íƒ ì‹œ ë‰´ìŠ¤ ì²˜ë¦¬
def handle_news_selection(selected_location, table_name, label):
    if selected_location != "ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”":
        with st.expander(f"ğŸ—ºï¸ {label} ({selected_location}) ë‰´ìŠ¤ ë³´ê¸°", expanded=True):
            with st.spinner("ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ìš”ì•½í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
                news_list = search_news(selected_location)
                if news_list:
                    save_news_to_db(news_list, table_name)
                    process_news_from_db(table_name)


# ë‰´ìŠ¤ ì²˜ë¦¬ í•¨ìˆ˜
def process_news_from_db(table_name: str):
    urls = []
    conn = None

    try:
        conn = get_db_connection()
        if conn:
            with conn.cursor() as cur:
                cur.execute(f"SELECT originallink FROM {table_name}")
                urls = [r[0] for r in cur.fetchall()]
    except Exception as e:
        st.error(f"DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
    finally:
        if conn:
            conn.close()

    if not urls:
        st.warning(f"{table_name}ì— ì €ì¥ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ì¤‘ë³µ ì œê±°
        urls = list(dict.fromkeys(urls))

        for idx, url in enumerate(urls, start=1):
            with st.container():
                st.markdown(f"### ğŸ”— ë‰´ìŠ¤ #{idx}")
                st.markdown(f"[{url}]({url})")

                article = get_html_text(url)
                if not article:
                    st.error("â— ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
                    continue

                summary = summarize_text(article)

                if "ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." in summary:
                    st.warning(summary)
                    continue

                st.success("âœ¨ ìš”ì•½ ê²°ê³¼")
                st.write(summary)

                try:
                    tts = gTTS(text=summary, lang="ko")
                    buf = BytesIO()
                    tts.write_to_fp(buf)
                    buf.seek(0)
                    st.audio(buf, format="audio/mp3")
                except Exception as e:
                    st.error(f"TTS ìƒì„± ì‹¤íŒ¨: {str(e)}")

            st.markdown("---")

# ë“œë¡­ë‹¤ìš´ë³„ ë‰´ìŠ¤ ì‹¤í–‰
handle_news_selection(departure, "departure_news", "ì¶œë°œì§€")
handle_news_selection(transit, "transit_news", "ê²½ìœ ì§€")
handle_news_selection(destination, "destination_news", "ëª©ì ì§€")

