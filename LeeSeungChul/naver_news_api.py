# --- í•„ìš”í•œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ---
import os
from dotenv import load_dotenv
import requests
import mysql.connector
from kor_location_map import find_location
from datetime import datetime
import time

# --- .env íŒŒì¼ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° ---
load_dotenv()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# --- ì‚¬ê³ ìœ í˜• í‚¤ì›Œë“œ ëª©ë¡ ---
accident_keywords = ["ìŒì£¼", "ê³¼ì†", "ì‹ í˜¸ìœ„ë°˜", "ë³´ë³µìš´ì „", "ë¬´ë©´í—ˆ", "ë¬´ë‹¨íš¡ë‹¨"]

# --- ìˆ˜ì§‘í•  í‚¤ì›Œë“œ ëª©ë¡ ---
keywords = [
    "êµí†µì‚¬ê³ ",
    "ìŒì£¼ìš´ì „",
    "ì‹ í˜¸ìœ„ë°˜",
    "ê³¼ì†ì‚¬ê³ ",
    "ë³´ë³µìš´ì „",
    "ë¬´ë©´í—ˆìš´ì „",
    "ë¬´ë‹¨íš¡ë‹¨ ì‚¬ê³ "
]

# --- ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ í•¨ìˆ˜ ---
def crawl_naver_news(keyword, start_num, max_count=100):
    url = "https://openapi.naver.com/v1/search/news.json"

    params = {
        "query": keyword,
        "display": max_count,
        "start": start_num,
        "sort": "date"
    }

    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }

    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        result = response.json()
        items = result.get("items", [])

        news_list = []
        for item in items:
            title = item.get("title", "").replace("<b>", "").replace("</b>", "")
            link = item.get("link", "")
            description = item.get("description", "").replace("<b>", "").replace("</b>", "")
            pubDate = item.get("pubDate", "")  # ë°œí–‰ì¼

            # pubDateë¥¼ accident_dateë¡œ ë³€í™˜
            try:
                pub_date_obj = datetime.strptime(pubDate, '%a, %d %b %Y %H:%M:%S %z')
                accident_date = pub_date_obj.strftime('%Y-%m-%d')
            except Exception as e:
                print(f"â— ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                accident_date = None

            news_list.append({
                "title": title,
                "link": link,
                "description": description,
                "accident_date": accident_date
            })

        return news_list
    else:
        print(f"â— ì—ëŸ¬ ë°œìƒ: {response.status_code}")
        return []

# --- ì‚¬ê³  ìœ í˜• ì°¾ê¸° í•¨ìˆ˜ ---
def find_accident_type(text):
    for keyword in accident_keywords:
        if keyword in text:
            return keyword
    return None

# --- ì™„ì„±ëœ ë‰´ìŠ¤ë§Œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ---
def save_news_to_db(news_list):
    config = {
        "host": "localhost",
        "user": "team5",
        "password": "teamteam5",
        "database": "team5_database"
    }

    conn = mysql.connector.connect(**config)
    cursor = conn.cursor()

    inserted_count = 0

    for news in news_list:
        title = news["title"]
        link = news["link"]
        description = news["description"]
        accident_date = news["accident_date"]

        combined_text = title + " " + description

        loc1, loc2 = find_location(combined_text)
        accident_type = find_accident_type(combined_text)

        # ğŸ”¥ location1, location2, accident_type, accident_dateê°€ ëª¨ë‘ ìˆì„ ë•Œë§Œ ì €ì¥
        if loc1 and loc2 and accident_type and accident_date:
            sql = """
                INSERT INTO car_accident_naver_news 
                (title, link, description, accident_location1, accident_location2, accident_type, accident_date)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """
            values = (title, link, description, loc1, loc2, accident_type, accident_date)

            try:
                cursor.execute(sql, values)
                inserted_count += 1
            except Exception as e:
                print(f"â— ì €ì¥ ì‹¤íŒ¨: {title} ì—ëŸ¬: {e}")

    conn.commit()
    print(f"âœ… DBì— ì €ì¥ëœ ë‰´ìŠ¤ ìˆ˜: {inserted_count}ê°œ")

    cursor.close()
    conn.close()

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    all_news = []

    for keyword in keywords:
        for page in range(10):  # í‚¤ì›Œë“œë‹¹ 5í˜ì´ì§€ (100ê°œ * 5 = ìµœëŒ€ 500ê°œ)
            start_num = page * 100 + 1
            news_list = crawl_naver_news(keyword, start_num, max_count=100)
            if not news_list:
                break
            all_news.extend(news_list)

    print(f"ì´ ìˆ˜ì§‘í•œ ë‰´ìŠ¤ ê°œìˆ˜(ê°€ê³µ ì „): {len(all_news)}ê°œ")

    # ê°€ê³µí•´ì„œ ì €ì¥
    save_news_to_db(all_news)
